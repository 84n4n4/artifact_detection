{
  "train_samples": 200000,
  "params": "{'charrep__repl_all_caps': False, 'vect__ngram_range': (1, 3), 'vect__stop_words': None, 'vect__lowercase': False}",
  "perf_train_runtime": 51.20854549399883,
  "model_size": 42.2057294921875,
  "seed": 42,
  "train_frac": 200000,
  "man_validation_samples_cpp_researcher_1": 5416,
  "classification_report_cpp_researcher_1": "{\"artifact\": {\"precision\": 0.9832163564235581, \"recall\": 0.8689320388349514, \"f1-score\": 0.9225483178239082, \"support\": 3708}, \"text\": {\"precision\": 0.7727910238429172, \"recall\": 0.9677985948477752, \"f1-score\": 0.8593709383935534, \"support\": 1708}, \"accuracy\": 0.9001107828655834, \"macro avg\": {\"precision\": 0.8780036901332376, \"recall\": 0.9183653168413632, \"f1-score\": 0.8909596281087309, \"support\": 5416}, \"weighted avg\": {\"precision\": 0.9168562256909631, \"recall\": 0.9001107828655834, \"f1-score\": 0.9026245799976441, \"support\": 5416}}",
  "macro_f1_cpp_researcher_1": 0.8909596281087309,
  "roc-auc_cpp_researcher_1": 0.9183653168413634,
  "perf_predict_runtime_cpp_researcher_1": 0.47279928300122265,
  "timeit_runtime_cpp_researcher_1": 0.4718094353000197,
  "man_validation_samples_cpp_researcher_2": 5418,
  "classification_report_cpp_researcher_2": "{\"artifact\": {\"precision\": 0.9811550151975684, \"recall\": 0.8679752621672493, \"f1-score\": 0.9211014410044229, \"support\": 3719}, \"text\": {\"precision\": 0.769266917293233, \"recall\": 0.9635079458505003, \"f1-score\": 0.8555003919519205, \"support\": 1699}, \"accuracy\": 0.8979328165374677, \"macro avg\": {\"precision\": 0.8752109662454007, \"recall\": 0.9157416040088748, \"f1-score\": 0.8883009164781717, \"support\": 5418}, \"weighted avg\": {\"precision\": 0.9147102240680989, \"recall\": 0.8979328165374677, \"f1-score\": 0.9005299787784721, \"support\": 5418}}",
  "macro_f1_cpp_researcher_2": 0.8883009164781717,
  "roc-auc_cpp_researcher_2": 0.9157416040088748,
  "perf_predict_runtime_cpp_researcher_2": 0.4805091859998356,
  "timeit_runtime_cpp_researcher_2": 0.4821768155998143,
  "man_validation_samples_java_researcher_1": 6575,
  "classification_report_java_researcher_1": "{\"artifact\": {\"precision\": 0.9887184115523465, \"recall\": 0.9347269624573379, \"f1-score\": 0.9609649122807018, \"support\": 4688}, \"text\": {\"precision\": 0.8572095193653756, \"recall\": 0.9735029146793853, \"f1-score\": 0.9116625310173697, \"support\": 1887}, \"accuracy\": 0.9458555133079848, \"macro avg\": {\"precision\": 0.9229639654588611, \"recall\": 0.9541149385683616, \"f1-score\": 0.9363137216490358, \"support\": 6575}, \"weighted avg\": {\"precision\": 0.9509758595284964, \"recall\": 0.9458555133079848, \"f1-score\": 0.9468153163196512, \"support\": 6575}}",
  "macro_f1_java_researcher_1": 0.9363137216490358,
  "roc-auc_java_researcher_1": 0.9541149385683616,
  "perf_predict_runtime_java_researcher_1": 0.45935103000010713,
  "timeit_runtime_java_researcher_1": 0.45920012229980783,
  "man_validation_samples_java_researcher_2": 6578,
  "classification_report_java_researcher_2": "{\"artifact\": {\"precision\": 0.9873703202525936, \"recall\": 0.9370719178082192, \"f1-score\": 0.9615638040852186, \"support\": 4672}, \"text\": {\"precision\": 0.8628731343283582, \"recall\": 0.9706190975865687, \"f1-score\": 0.9135802469135802, \"support\": 1906}, \"accuracy\": 0.946792338096686, \"macro avg\": {\"precision\": 0.9251217272904759, \"recall\": 0.953845507697394, \"f1-score\": 0.9375720254993993, \"support\": 6578}, \"weighted avg\": {\"precision\": 0.951296796936754, \"recall\": 0.946792338096686, \"f1-score\": 0.9476603896782344, \"support\": 6578}}",
  "macro_f1_java_researcher_2": 0.9375720254993993,
  "roc-auc_java_researcher_2": 0.953845507697394,
  "perf_predict_runtime_java_researcher_2": 0.45901351399879786,
  "timeit_runtime_java_researcher_2": 0.46070779399997264,
  "man_validation_samples_javascript_researcher_1": 5170,
  "classification_report_javascript_researcher_1": "{\"artifact\": {\"precision\": 0.9628192693178145, \"recall\": 0.9191358024691358, \"f1-score\": 0.9404705510816359, \"support\": 3240}, \"text\": {\"precision\": 0.8738565238324506, \"recall\": 0.9404145077720207, \"f1-score\": 0.9059146493636137, \"support\": 1930}, \"accuracy\": 0.9270793036750483, \"macro avg\": {\"precision\": 0.9183378965751325, \"recall\": 0.9297751551205783, \"f1-score\": 0.9231926002226247, \"support\": 5170}, \"weighted avg\": {\"precision\": 0.9296088053358508, \"recall\": 0.9270793036750483, \"f1-score\": 0.9275705722971518, \"support\": 5170}}",
  "macro_f1_javascript_researcher_1": 0.9231926002226247,
  "roc-auc_javascript_researcher_1": 0.9297751551205783,
  "perf_predict_runtime_javascript_researcher_1": 0.307392450999032,
  "timeit_runtime_javascript_researcher_1": 0.30588283430006413,
  "man_validation_samples_javascript_researcher_2": 5167,
  "classification_report_javascript_researcher_2": "{\"artifact\": {\"precision\": 0.9605050178051149, \"recall\": 0.92, \"f1-score\": 0.9398162812796959, \"support\": 3225}, \"text\": {\"precision\": 0.875842155919153, \"recall\": 0.937178166838311, \"f1-score\": 0.9054726368159203, \"support\": 1942}, \"accuracy\": 0.9264563576543449, \"macro avg\": {\"precision\": 0.9181735868621339, \"recall\": 0.9285890834191555, \"f1-score\": 0.922644459047808, \"support\": 5167}, \"weighted avg\": {\"precision\": 0.9286847588961661, \"recall\": 0.9264563576543449, \"f1-score\": 0.9269083351700284, \"support\": 5167}}",
  "macro_f1_javascript_researcher_2": 0.922644459047808,
  "roc-auc_javascript_researcher_2": 0.9285890834191556,
  "perf_predict_runtime_javascript_researcher_2": 0.3047535320001771,
  "timeit_runtime_javascript_researcher_2": 0.3067412500997307,
  "man_validation_samples_php_researcher_1": 6190,
  "classification_report_php_researcher_1": "{\"artifact\": {\"precision\": 0.9781149693609571, \"recall\": 0.7931850449597728, \"f1-score\": 0.8759963413040637, \"support\": 4226}, \"text\": {\"precision\": 0.683677162504524, \"recall\": 0.9618126272912424, \"f1-score\": 0.7992384176010153, \"support\": 1964}, \"accuracy\": 0.8466882067851373, \"macro avg\": {\"precision\": 0.8308960659327406, \"recall\": 0.8774988361255076, \"f1-score\": 0.8376173794525394, \"support\": 6190}, \"weighted avg\": {\"precision\": 0.8846939915473813, \"recall\": 0.8466882067851373, \"f1-score\": 0.8516421309401239, \"support\": 6190}}",
  "macro_f1_php_researcher_1": 0.8376173794525394,
  "roc-auc_php_researcher_1": 0.8774988361255076,
  "perf_predict_runtime_php_researcher_1": 0.36419618599757086,
  "timeit_runtime_php_researcher_1": 0.3638614411000162,
  "man_validation_samples_php_researcher_2": 6189,
  "classification_report_php_researcher_2": "{\"artifact\": {\"precision\": 0.9785672342924251, \"recall\": 0.7939494997617913, \"f1-score\": 0.8766438716465017, \"support\": 4198}, \"text\": {\"precision\": 0.6891843334531081, \"recall\": 0.9633350075339026, \"f1-score\": 0.8035190615835777, \"support\": 1991}, \"accuracy\": 0.8484407820326385, \"macro avg\": {\"precision\": 0.8338757838727666, \"recall\": 0.8786422536478469, \"f1-score\": 0.8400814666150397, \"support\": 6189}, \"weighted avg\": {\"precision\": 0.8854728158773209, \"recall\": 0.8484407820326385, \"f1-score\": 0.853119635609132, \"support\": 6189}}",
  "macro_f1_php_researcher_2": 0.8400814666150397,
  "roc-auc_php_researcher_2": 0.8786422536478469,
  "perf_predict_runtime_php_researcher_2": 0.36521263499889756,
  "timeit_runtime_php_researcher_2": 0.36525800750023335,
  "man_validation_samples_python_researcher_1": 10901,
  "classification_report_python_researcher_1": "{\"artifact\": {\"precision\": 0.9808985741189131, \"recall\": 0.8741309038599856, \"f1-score\": 0.9244421906693712, \"support\": 8342}, \"text\": {\"precision\": 0.6971445053360253, \"recall\": 0.9445095740523642, \"f1-score\": 0.8021905077995354, \"support\": 2559}, \"accuracy\": 0.8906522337400239, \"macro avg\": {\"precision\": 0.8390215397274692, \"recall\": 0.909320238956175, \"f1-score\": 0.8633163492344533, \"support\": 10901}, \"weighted avg\": {\"precision\": 0.9142875602655595, \"recall\": 0.8906522337400239, \"f1-score\": 0.8957437174592152, \"support\": 10901}}",
  "macro_f1_python_researcher_1": 0.8633163492344533,
  "roc-auc_python_researcher_1": 0.9093202389561749,
  "perf_predict_runtime_python_researcher_1": 0.6607222429993271,
  "timeit_runtime_python_researcher_1": 0.6655684065000969,
  "man_validation_samples_python_researcher_2": 10983,
  "classification_report_python_researcher_2": "{\"artifact\": {\"precision\": 0.9771033013844516, \"recall\": 0.8710098492939362, \"f1-score\": 0.9210113557939646, \"support\": 8427}, \"text\": {\"precision\": 0.6868337654854508, \"recall\": 0.9327073552425665, \"f1-score\": 0.79110668657707, \"support\": 2556}, \"accuracy\": 0.8853682964581626, \"macro avg\": {\"precision\": 0.8319685334349511, \"recall\": 0.9018586022682513, \"f1-score\": 0.8560590211855172, \"support\": 10983}, \"weighted avg\": {\"precision\": 0.9095508172036406, \"recall\": 0.8853682964581626, \"f1-score\": 0.8907795125345289, \"support\": 10983}}",
  "macro_f1_python_researcher_2": 0.8560590211855172,
  "roc-auc_python_researcher_2": 0.9018586022682514,
  "perf_predict_runtime_python_researcher_2": 0.6684861950016057,
  "timeit_runtime_python_researcher_2": 0.6666952498002502
}