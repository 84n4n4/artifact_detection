{
  "train_samples": 200000,
  "params": "{'charrep__repl_all_caps': False, 'vect__ngram_range': (1, 3), 'vect__stop_words': None, 'vect__lowercase': False}",
  "perf_train_runtime": 82.83172637700045,
  "model_size": 33.7076630859375,
  "seed": 42,
  "train_frac": 200000,
  "man_validation_samples_cpp_researcher_1": 5416,
  "classification_report_cpp_researcher_1": "{\"artifact\": {\"precision\": 0.984047019311503, \"recall\": 0.948220064724919, \"f1-score\": 0.9658014009064689, \"support\": 3708}, \"text\": {\"precision\": 0.8958220293000543, \"recall\": 0.9666276346604216, \"f1-score\": 0.9298789073500423, \"support\": 1708}, \"accuracy\": 0.9540251107828656, \"macro avg\": {\"precision\": 0.9399345243057786, \"recall\": 0.9574238496926704, \"f1-score\": 0.9478401541282556, \"support\": 5416}, \"weighted avg\": {\"precision\": 0.956224219655012, \"recall\": 0.9540251107828656, \"f1-score\": 0.9544728154200626, \"support\": 5416}}",
  "macro_f1_cpp_researcher_1": 0.9478401541282556,
  "roc-auc_cpp_researcher_1": 0.9574238496926702,
  "perf_predict_runtime_cpp_researcher_1": 0.49180297600105405,
  "timeit_runtime_cpp_researcher_1": 0.5052316576000522,
  "man_validation_samples_cpp_researcher_2": 5418,
  "classification_report_cpp_researcher_2": "{\"artifact\": {\"precision\": 0.9815539407490218, \"recall\": 0.9443398763108363, \"f1-score\": 0.9625873646704125, \"support\": 3719}, \"text\": {\"precision\": 0.8875, \"recall\": 0.9611536197763391, \"f1-score\": 0.9228595648488274, \"support\": 1699}, \"accuracy\": 0.9496124031007752, \"macro avg\": {\"precision\": 0.9345269703745109, \"recall\": 0.9527467480435876, \"f1-score\": 0.9427234647596199, \"support\": 5418}, \"weighted avg\": {\"precision\": 0.9520600970183853, \"recall\": 0.9496124031007752, \"f1-score\": 0.9501293484472908, \"support\": 5418}}",
  "macro_f1_cpp_researcher_2": 0.9427234647596199,
  "roc-auc_cpp_researcher_2": 0.9527467480435876,
  "perf_predict_runtime_cpp_researcher_2": 0.49836145199878956,
  "timeit_runtime_cpp_researcher_2": 0.5108801129001221,
  "man_validation_samples_java_researcher_1": 6575,
  "classification_report_java_researcher_1": "{\"artifact\": {\"precision\": 0.9819569743233866, \"recall\": 0.9055034129692833, \"f1-score\": 0.9421817778271002, \"support\": 4688}, \"text\": {\"precision\": 0.8032859680284192, \"recall\": 0.958664546899841, \"f1-score\": 0.8741241845856488, \"support\": 1887}, \"accuracy\": 0.9207604562737642, \"macro avg\": {\"precision\": 0.8926214711759028, \"recall\": 0.9320839799345622, \"f1-score\": 0.9081529812063744, \"support\": 6575}, \"weighted avg\": {\"precision\": 0.9306790748741695, \"recall\": 0.9207604562737642, \"f1-score\": 0.9226495073409225, \"support\": 6575}}",
  "macro_f1_java_researcher_1": 0.9081529812063744,
  "roc-auc_java_researcher_1": 0.9320839799345623,
  "perf_predict_runtime_java_researcher_1": 0.49211285800083715,
  "timeit_runtime_java_researcher_1": 0.48193335870000736,
  "man_validation_samples_java_researcher_2": 6578,
  "classification_report_java_researcher_2": "{\"artifact\": {\"precision\": 0.9787332408691632, \"recall\": 0.90625, \"f1-score\": 0.9410980217826183, \"support\": 4672}, \"text\": {\"precision\": 0.80550621669627, \"recall\": 0.9517313746065058, \"f1-score\": 0.8725348725348725, \"support\": 1906}, \"accuracy\": 0.9194283976892672, \"macro avg\": {\"precision\": 0.8921197287827166, \"recall\": 0.928990687303253, \"f1-score\": 0.9068164471587454, \"support\": 6578}, \"weighted avg\": {\"precision\": 0.9285400654247221, \"recall\": 0.9194283976892672, \"f1-score\": 0.9212315939221434, \"support\": 6578}}",
  "macro_f1_java_researcher_2": 0.9068164471587454,
  "roc-auc_java_researcher_2": 0.928990687303253,
  "perf_predict_runtime_java_researcher_2": 0.4727955419984937,
  "timeit_runtime_java_researcher_2": 0.4778116931000113,
  "man_validation_samples_javascript_researcher_1": 5170,
  "classification_report_javascript_researcher_1": "{\"artifact\": {\"precision\": 0.9712625121084921, \"recall\": 0.928395061728395, \"f1-score\": 0.949345115985482, \"support\": 3240}, \"text\": {\"precision\": 0.8880849011095031, \"recall\": 0.9538860103626943, \"f1-score\": 0.919810142393205, \"support\": 1930}, \"accuracy\": 0.9379110251450677, \"macro avg\": {\"precision\": 0.9296737066089976, \"recall\": 0.9411405360455447, \"f1-score\": 0.9345776291893435, \"support\": 5170}, \"weighted avg\": {\"precision\": 0.9402116824705717, \"recall\": 0.9379110251450677, \"f1-score\": 0.9383194875458118, \"support\": 5170}}",
  "macro_f1_javascript_researcher_1": 0.9345776291893435,
  "roc-auc_javascript_researcher_1": 0.9411405360455447,
  "perf_predict_runtime_javascript_researcher_1": 0.3222559629994066,
  "timeit_runtime_javascript_researcher_1": 0.3203377076999459,
  "man_validation_samples_javascript_researcher_2": 5167,
  "classification_report_javascript_researcher_2": "{\"artifact\": {\"precision\": 0.9709020368574199, \"recall\": 0.9311627906976744, \"f1-score\": 0.9506172839506172, \"support\": 3225}, \"text\": {\"precision\": 0.892960462873674, \"recall\": 0.9536560247167868, \"f1-score\": 0.9223107569721114, \"support\": 1942}, \"accuracy\": 0.939616798916199, \"macro avg\": {\"precision\": 0.931931249865547, \"recall\": 0.9424094077072306, \"f1-score\": 0.9364640204613643, \"support\": 5167}, \"weighted avg\": {\"precision\": 0.9416079519577809, \"recall\": 0.939616798916199, \"f1-score\": 0.9399783686434258, \"support\": 5167}}",
  "macro_f1_javascript_researcher_2": 0.9364640204613643,
  "roc-auc_javascript_researcher_2": 0.9424094077072306,
  "perf_predict_runtime_javascript_researcher_2": 0.3183613159999368,
  "timeit_runtime_javascript_researcher_2": 0.32042875750012173,
  "man_validation_samples_php_researcher_1": 6190,
  "classification_report_php_researcher_1": "{\"artifact\": {\"precision\": 0.9820700896495518, \"recall\": 0.855418835778514, \"f1-score\": 0.9143796635892247, \"support\": 4226}, \"text\": {\"precision\": 0.7564766839378239, \"recall\": 0.9663951120162932, \"f1-score\": 0.848647440196736, \"support\": 1964}, \"accuracy\": 0.8906300484652666, \"macro avg\": {\"precision\": 0.8692733867936878, \"recall\": 0.9109069738974036, \"f1-score\": 0.8815135518929804, \"support\": 6190}, \"weighted avg\": {\"precision\": 0.9104924727161375, \"recall\": 0.8906300484652666, \"f1-score\": 0.8935237529684092, \"support\": 6190}}",
  "macro_f1_php_researcher_1": 0.8815135518929804,
  "roc-auc_php_researcher_1": 0.9109069738974036,
  "perf_predict_runtime_php_researcher_1": 0.3850265800010675,
  "timeit_runtime_php_researcher_1": 0.381729525999981,
  "man_validation_samples_php_researcher_2": 6189,
  "classification_report_php_researcher_2": "{\"artifact\": {\"precision\": 0.9751887810140237, \"recall\": 0.8613625535969509, \"f1-score\": 0.9147482924361244, \"support\": 4198}, \"text\": {\"precision\": 0.7654171704957679, \"recall\": 0.9537920642893019, \"f1-score\": 0.8492844364937387, \"support\": 1991}, \"accuracy\": 0.8910971077718532, \"macro avg\": {\"precision\": 0.8703029757548958, \"recall\": 0.9075773089431264, \"f1-score\": 0.8820163644649316, \"support\": 6189}, \"weighted avg\": {\"precision\": 0.9077052979728463, \"recall\": 0.8910971077718532, \"f1-score\": 0.8936885837301478, \"support\": 6189}}",
  "macro_f1_php_researcher_2": 0.8820163644649316,
  "roc-auc_php_researcher_2": 0.9075773089431264,
  "perf_predict_runtime_php_researcher_2": 0.3856192220009689,
  "timeit_runtime_php_researcher_2": 0.38525507600006675,
  "man_validation_samples_python_researcher_1": 10901,
  "classification_report_python_researcher_1": "{\"artifact\": {\"precision\": 0.9858657243816255, \"recall\": 0.8695756413330137, \"f1-score\": 0.9240764331210192, \"support\": 8342}, \"text\": {\"precision\": 0.6929156082416031, \"recall\": 0.9593591246580696, \"f1-score\": 0.8046542117338578, \"support\": 2559}, \"accuracy\": 0.8906522337400239, \"macro avg\": {\"precision\": 0.8393906663116143, \"recall\": 0.9144673829955416, \"f1-score\": 0.8643653224274385, \"support\": 10901}, \"weighted avg\": {\"precision\": 0.9170959466362519, \"recall\": 0.8906522337400239, \"f1-score\": 0.8960421734632129, \"support\": 10901}}",
  "macro_f1_python_researcher_1": 0.8643653224274385,
  "roc-auc_python_researcher_1": 0.9144673829955416,
  "perf_predict_runtime_python_researcher_1": 0.6969027969989838,
  "timeit_runtime_python_researcher_1": 0.6984348632999172,
  "man_validation_samples_python_researcher_2": 10983,
  "classification_report_python_researcher_2": "{\"artifact\": {\"precision\": 0.9813172043010753, \"recall\": 0.8663818678058621, \"f1-score\": 0.9202747841431903, \"support\": 8427}, \"text\": {\"precision\": 0.6821902342647473, \"recall\": 0.9456181533646323, \"f1-score\": 0.792588949008034, \"support\": 2556}, \"accuracy\": 0.8848219976327051, \"macro avg\": {\"precision\": 0.8317537192829113, \"recall\": 0.9060000105852473, \"f1-score\": 0.8564318665756121, \"support\": 10983}, \"weighted avg\": {\"precision\": 0.9117033888214383, \"recall\": 0.8848219976327051, \"f1-score\": 0.8905593152726213, \"support\": 10983}}",
  "macro_f1_python_researcher_2": 0.8564318665756121,
  "roc-auc_python_researcher_2": 0.9060000105852472,
  "perf_predict_runtime_python_researcher_2": 0.6942667710009118,
  "timeit_runtime_python_researcher_2": 0.70002402909995
}