{
  "train_samples": 200000,
  "params": "{'charrep__repl_all_caps': False, 'vect__ngram_range': (1, 3), 'vect__stop_words': None, 'vect__lowercase': False}",
  "perf_train_runtime": 84.68376779000027,
  "model_size": 34.38151171875,
  "seed": 42,
  "train_frac": 200000,
  "man_validation_samples_cpp_researcher_1": 5416,
  "classification_report_cpp_researcher_1": "{\"artifact\": {\"precision\": 0.9703450242372398, \"recall\": 0.9177454153182308, \"f1-score\": 0.9433125433125432, \"support\": 3708}, \"text\": {\"precision\": 0.8402304871660555, \"recall\": 0.9391100702576113, \"f1-score\": 0.8869228642521428, \"support\": 1708}, \"accuracy\": 0.9244830132939439, \"macro avg\": {\"precision\": 0.9052877557016477, \"recall\": 0.9284277427879211, \"f1-score\": 0.915117703782343, \"support\": 5416}, \"weighted avg\": {\"precision\": 0.9293118578196654, \"recall\": 0.9244830132939439, \"f1-score\": 0.9255293875084141, \"support\": 5416}}",
  "macro_f1_cpp_researcher_1": 0.915117703782343,
  "roc-auc_cpp_researcher_1": 0.9284277427879211,
  "perf_predict_runtime_cpp_researcher_1": 0.4705327640003816,
  "timeit_runtime_cpp_researcher_1": 0.4729845339003077,
  "man_validation_samples_cpp_researcher_2": 5418,
  "classification_report_cpp_researcher_2": "{\"artifact\": {\"precision\": 0.9683490162532079, \"recall\": 0.9131486958859909, \"f1-score\": 0.9399391087738722, \"support\": 3719}, \"text\": {\"precision\": 0.8309785452642595, \"recall\": 0.9346674514420247, \"f1-score\": 0.8797783933518004, \"support\": 1699}, \"accuracy\": 0.9198966408268734, \"macro avg\": {\"precision\": 0.8996637807587338, \"recall\": 0.9239080736640077, \"f1-score\": 0.9098587510628363, \"support\": 5418}, \"weighted avg\": {\"precision\": 0.9252717866093867, \"recall\": 0.9198966408268734, \"f1-score\": 0.921073650024869, \"support\": 5418}}",
  "macro_f1_cpp_researcher_2": 0.9098587510628363,
  "roc-auc_cpp_researcher_2": 0.9239080736640078,
  "perf_predict_runtime_cpp_researcher_2": 0.4829596599993238,
  "timeit_runtime_cpp_researcher_2": 0.48728445310007373,
  "man_validation_samples_java_researcher_1": 6575,
  "classification_report_java_researcher_1": "{\"artifact\": {\"precision\": 0.975914362176628, \"recall\": 0.9334470989761092, \"f1-score\": 0.954208460532054, \"support\": 4688}, \"text\": {\"precision\": 0.8507890961262554, \"recall\": 0.9427662957074722, \"f1-score\": 0.894419306184012, \"support\": 1887}, \"accuracy\": 0.9361216730038023, \"macro avg\": {\"precision\": 0.9133517291514417, \"recall\": 0.9381066973417906, \"f1-score\": 0.924313883358033, \"support\": 6575}, \"weighted avg\": {\"precision\": 0.9400038865816389, \"recall\": 0.9361216730038023, \"f1-score\": 0.9370492005693536, \"support\": 6575}}",
  "macro_f1_java_researcher_1": 0.924313883358033,
  "roc-auc_java_researcher_1": 0.9381066973417906,
  "perf_predict_runtime_java_researcher_1": 0.46502478099864675,
  "timeit_runtime_java_researcher_1": 0.45454461699991955,
  "man_validation_samples_java_researcher_2": 6578,
  "classification_report_java_researcher_2": "{\"artifact\": {\"precision\": 0.9745932694450635, \"recall\": 0.9360017123287672, \"f1-score\": 0.954907741019762, \"support\": 4672}, \"text\": {\"precision\": 0.8570062171209948, \"recall\": 0.9401888772298006, \"f1-score\": 0.8966725043782836, \"support\": 1906}, \"accuracy\": 0.9372149589540893, \"macro avg\": {\"precision\": 0.9157997432830292, \"recall\": 0.9380952947792839, \"f1-score\": 0.9257901226990228, \"support\": 6578}, \"weighted avg\": {\"precision\": 0.9405219830769158, \"recall\": 0.9372149589540893, \"f1-score\": 0.9380338643036389, \"support\": 6578}}",
  "macro_f1_java_researcher_2": 0.9257901226990228,
  "roc-auc_java_researcher_2": 0.9380952947792839,
  "perf_predict_runtime_java_researcher_2": 0.4549934739989112,
  "timeit_runtime_java_researcher_2": 0.4574636745001044,
  "man_validation_samples_javascript_researcher_1": 5170,
  "classification_report_javascript_researcher_1": "{\"artifact\": {\"precision\": 0.964713499514406, \"recall\": 0.9197530864197531, \"f1-score\": 0.9416969505451098, \"support\": 3240}, \"text\": {\"precision\": 0.8750600672753484, \"recall\": 0.9435233160621762, \"f1-score\": 0.9080029917726252, \"support\": 1930}, \"accuracy\": 0.9286266924564797, \"macro avg\": {\"precision\": 0.9198867833948772, \"recall\": 0.9316382012409646, \"f1-score\": 0.9248499711588676, \"support\": 5170}, \"weighted avg\": {\"precision\": 0.931245196957079, \"recall\": 0.9286266924564797, \"f1-score\": 0.9291187415642791, \"support\": 5170}}",
  "macro_f1_javascript_researcher_1": 0.9248499711588676,
  "roc-auc_javascript_researcher_1": 0.9316382012409646,
  "perf_predict_runtime_javascript_researcher_1": 0.30937537100180634,
  "timeit_runtime_javascript_researcher_1": 0.301969525900131,
  "man_validation_samples_javascript_researcher_2": 5167,
  "classification_report_javascript_researcher_2": "{\"artifact\": {\"precision\": 0.9620868438107583, \"recall\": 0.9206201550387597, \"f1-score\": 0.9408968467754715, \"support\": 3225}, \"text\": {\"precision\": 0.8769822200864968, \"recall\": 0.9397528321318228, \"f1-score\": 0.9072831220482227, \"support\": 1942}, \"accuracy\": 0.9278111089607122, \"macro avg\": {\"precision\": 0.9195345319486276, \"recall\": 0.9301864935852913, \"f1-score\": 0.9240899844118471, \"support\": 5167}, \"weighted avg\": {\"precision\": 0.9301005501640549, \"recall\": 0.9278111089607122, \"f1-score\": 0.9282632386043244, \"support\": 5167}}",
  "macro_f1_javascript_researcher_2": 0.9240899844118471,
  "roc-auc_javascript_researcher_2": 0.9301864935852913,
  "perf_predict_runtime_javascript_researcher_2": 0.298956629998429,
  "timeit_runtime_javascript_researcher_2": 0.30342276789997413,
  "man_validation_samples_php_researcher_1": 6190,
  "classification_report_php_researcher_1": "{\"artifact\": {\"precision\": 0.9778085991678225, \"recall\": 0.8341221012778041, \"f1-score\": 0.9002681649853147, \"support\": 4226}, \"text\": {\"precision\": 0.7288201160541586, \"recall\": 0.9592668024439919, \"f1-score\": 0.8283139151461859, \"support\": 1964}, \"accuracy\": 0.8738287560581584, \"macro avg\": {\"precision\": 0.8533143576109905, \"recall\": 0.8966944518608979, \"f1-score\": 0.8642910400657503, \"support\": 6190}, \"weighted avg\": {\"precision\": 0.8988080529908863, \"recall\": 0.8738287560581584, \"f1-score\": 0.8774380928231097, \"support\": 6190}}",
  "macro_f1_php_researcher_1": 0.8642910400657503,
  "roc-auc_php_researcher_1": 0.8966944518608979,
  "perf_predict_runtime_php_researcher_1": 0.3654259639988595,
  "timeit_runtime_php_researcher_1": 0.36725529590003136,
  "man_validation_samples_php_researcher_2": 6189,
  "classification_report_php_researcher_2": "{\"artifact\": {\"precision\": 0.9684035476718403, \"recall\": 0.8323010957598856, \"f1-score\": 0.895208813733026, \"support\": 4198}, \"text\": {\"precision\": 0.727237504843084, \"recall\": 0.9427423405323958, \"f1-score\": 0.8210848643919509, \"support\": 1991}, \"accuracy\": 0.8678300210050088, \"macro avg\": {\"precision\": 0.8478205262574622, \"recall\": 0.8875217181461408, \"f1-score\": 0.8581468390624885, \"support\": 6189}, \"weighted avg\": {\"precision\": 0.8908204823507782, \"recall\": 0.8678300210050088, \"f1-score\": 0.8713631547997444, \"support\": 6189}}",
  "macro_f1_php_researcher_2": 0.8581468390624885,
  "roc-auc_php_researcher_2": 0.8875217181461407,
  "perf_predict_runtime_php_researcher_2": 0.3634019970013469,
  "timeit_runtime_php_researcher_2": 0.3713541222998174,
  "man_validation_samples_python_researcher_1": 10901,
  "classification_report_python_researcher_1": "{\"artifact\": {\"precision\": 0.9869465751581213, \"recall\": 0.8791656677055862, \"f1-score\": 0.9299435744626895, \"support\": 8342}, \"text\": {\"precision\": 0.7095100864553314, \"recall\": 0.9620945681906995, \"f1-score\": 0.8167191905788688, \"support\": 2559}, \"accuracy\": 0.8986331529217503, \"macro avg\": {\"precision\": 0.8482283308067264, \"recall\": 0.9206301179481429, \"f1-score\": 0.8733313825207791, \"support\": 10901}, \"weighted avg\": {\"precision\": 0.9218186075780427, \"recall\": 0.8986331529217503, \"f1-score\": 0.9033642516153639, \"support\": 10901}}",
  "macro_f1_python_researcher_1": 0.8733313825207791,
  "roc-auc_python_researcher_1": 0.9206301179481428,
  "perf_predict_runtime_python_researcher_1": 0.6853277890004392,
  "timeit_runtime_python_researcher_1": 0.6789739611998812,
  "man_validation_samples_python_researcher_2": 10983,
  "classification_report_python_researcher_2": "{\"artifact\": {\"precision\": 0.9833666001330672, \"recall\": 0.8769431588940311, \"f1-score\": 0.9271107765650483, \"support\": 8427}, \"text\": {\"precision\": 0.7009803921568627, \"recall\": 0.951095461658842, \"f1-score\": 0.8071049136786187, \"support\": 2556}, \"accuracy\": 0.894200127469726, \"macro avg\": {\"precision\": 0.842173496144965, \"recall\": 0.9140193102764365, \"f1-score\": 0.8671078451218335, \"support\": 10983}, \"weighted avg\": {\"precision\": 0.9176487500386322, \"recall\": 0.894200127469726, \"f1-score\": 0.8991826161773843, \"support\": 10983}}",
  "macro_f1_python_researcher_2": 0.8671078451218335,
  "roc-auc_python_researcher_2": 0.9140193102764365,
  "perf_predict_runtime_python_researcher_2": 0.6816906809981447,
  "timeit_runtime_python_researcher_2": 0.6763895203999709
}