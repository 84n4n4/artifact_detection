{
  "train_samples": 200000,
  "params": "{'charrep__repl_all_caps': False, 'vect__ngram_range': (1, 3), 'vect__stop_words': None, 'vect__lowercase': False}",
  "perf_train_runtime": 95.8308377659996,
  "model_size": 49.350439453125,
  "seed": 42,
  "train_frac": 200000,
  "man_validation_samples_cpp_researcher_1": 5416,
  "classification_report_cpp_researcher_1": "{\"artifact\": {\"precision\": 0.9815986198964922, \"recall\": 0.9207119741100324, \"f1-score\": 0.9501809073197885, \"support\": 3708}, \"text\": {\"precision\": 0.848297213622291, \"recall\": 0.9625292740046838, \"f1-score\": 0.9018102029621502, \"support\": 1708}, \"accuracy\": 0.9338995568685377, \"macro avg\": {\"precision\": 0.9149479167593917, \"recall\": 0.9416206240573581, \"f1-score\": 0.9259955551409693, \"support\": 5416}, \"weighted avg\": {\"precision\": 0.9395604363816593, \"recall\": 0.9338995568685377, \"f1-score\": 0.9349266305393517, \"support\": 5416}}",
  "macro_f1_cpp_researcher_1": 0.9259955551409693,
  "roc-auc_cpp_researcher_1": 0.9416206240573581,
  "perf_predict_runtime_cpp_researcher_1": 0.5419133300001704,
  "timeit_runtime_cpp_researcher_1": 0.5338896319000923,
  "man_validation_samples_cpp_researcher_2": 5418,
  "classification_report_cpp_researcher_2": "{\"artifact\": {\"precision\": 0.9781922525107604, \"recall\": 0.9166442592094649, \"f1-score\": 0.9464186563020544, \"support\": 3719}, \"text\": {\"precision\": 0.8396275219865494, \"recall\": 0.9552678045909359, \"f1-score\": 0.8937224669603524, \"support\": 1699}, \"accuracy\": 0.9287559985234404, \"macro avg\": {\"precision\": 0.908909887248655, \"recall\": 0.9359560319002004, \"f1-score\": 0.9200705616312034, \"support\": 5418}, \"weighted avg\": {\"precision\": 0.9347405217686721, \"recall\": 0.9287559985234404, \"f1-score\": 0.9298939561005867, \"support\": 5418}}",
  "macro_f1_cpp_researcher_2": 0.9200705616312034,
  "roc-auc_cpp_researcher_2": 0.9359560319002004,
  "perf_predict_runtime_cpp_researcher_2": 0.5261012900009518,
  "timeit_runtime_cpp_researcher_2": 0.5466225738999129,
  "man_validation_samples_java_researcher_1": 6575,
  "classification_report_java_researcher_1": "{\"artifact\": {\"precision\": 0.986468200270636, \"recall\": 0.9330204778156996, \"f1-score\": 0.9590002192501644, \"support\": 4688}, \"text\": {\"precision\": 0.8533395609528258, \"recall\": 0.9682034976152624, \"f1-score\": 0.9071499503475671, \"support\": 1887}, \"accuracy\": 0.9431178707224335, \"macro avg\": {\"precision\": 0.9199038806117309, \"recall\": 0.950611987715481, \"f1-score\": 0.9330750847988658, \"support\": 6575}, \"weighted avg\": {\"precision\": 0.9482607869789694, \"recall\": 0.9431178707224335, \"f1-score\": 0.9441193892244304, \"support\": 6575}}",
  "macro_f1_java_researcher_1": 0.9330750847988658,
  "roc-auc_java_researcher_1": 0.9506119877154809,
  "perf_predict_runtime_java_researcher_1": 0.5069758650006406,
  "timeit_runtime_java_researcher_1": 0.5043740204999267,
  "man_validation_samples_java_researcher_2": 6578,
  "classification_report_java_researcher_2": "{\"artifact\": {\"precision\": 0.9851217312894499, \"recall\": 0.9353595890410958, \"f1-score\": 0.9595959595959596, \"support\": 4672}, \"text\": {\"precision\": 0.8590102707749766, \"recall\": 0.9653725078698846, \"f1-score\": 0.9090909090909091, \"support\": 1906}, \"accuracy\": 0.9440559440559441, \"macro avg\": {\"precision\": 0.9220660010322133, \"recall\": 0.9503660484554902, \"f1-score\": 0.9343434343434343, \"support\": 6578}, \"weighted avg\": {\"precision\": 0.9485804658986645, \"recall\": 0.9440559440559441, \"f1-score\": 0.9449619331042256, \"support\": 6578}}",
  "macro_f1_java_researcher_2": 0.9343434343434343,
  "roc-auc_java_researcher_2": 0.9503660484554903,
  "perf_predict_runtime_java_researcher_2": 0.5066809320014727,
  "timeit_runtime_java_researcher_2": 0.4953102047998982,
  "man_validation_samples_javascript_researcher_1": 5170,
  "classification_report_javascript_researcher_1": "{\"artifact\": {\"precision\": 0.975523349436393, \"recall\": 0.9348765432098766, \"f1-score\": 0.9547675334909378, \"support\": 3240}, \"text\": {\"precision\": 0.897820823244552, \"recall\": 0.960621761658031, \"f1-score\": 0.9281602002503129, \"support\": 1930}, \"accuracy\": 0.9444874274661509, \"macro avg\": {\"precision\": 0.9366720863404725, \"recall\": 0.9477491524339539, \"f1-score\": 0.9414638668706253, \"support\": 5170}, \"weighted avg\": {\"precision\": 0.9465164102583943, \"recall\": 0.9444874274661509, \"f1-score\": 0.9448348152792538, \"support\": 5170}}",
  "macro_f1_javascript_researcher_1": 0.9414638668706253,
  "roc-auc_javascript_researcher_1": 0.9477491524339537,
  "perf_predict_runtime_javascript_researcher_1": 0.3345209960007196,
  "timeit_runtime_javascript_researcher_1": 0.33876959469998835,
  "man_validation_samples_javascript_researcher_2": 5167,
  "classification_report_javascript_researcher_2": "{\"artifact\": {\"precision\": 0.9754917768461786, \"recall\": 0.937984496124031, \"f1-score\": 0.9563705343028769, \"support\": 3225}, \"text\": {\"precision\": 0.9031945788964182, \"recall\": 0.96086508753862, \"f1-score\": 0.9311377245508983, \"support\": 1942}, \"accuracy\": 0.9465840913489453, \"macro avg\": {\"precision\": 0.9393431778712984, \"recall\": 0.9494247918313254, \"f1-score\": 0.9437541294268876, \"support\": 5167}, \"weighted avg\": {\"precision\": 0.9483191121629129, \"recall\": 0.9465840913489453, \"f1-score\": 0.9468868655321506, \"support\": 5167}}",
  "macro_f1_javascript_researcher_2": 0.9437541294268876,
  "roc-auc_javascript_researcher_2": 0.9494247918313254,
  "perf_predict_runtime_javascript_researcher_2": 0.34342235700023593,
  "timeit_runtime_javascript_researcher_2": 0.33772904229990675,
  "man_validation_samples_php_researcher_1": 6190,
  "classification_report_php_researcher_1": "{\"artifact\": {\"precision\": 0.987406216505895, \"recall\": 0.8719829626123994, \"f1-score\": 0.9261120884644383, \"support\": 4226}, \"text\": {\"precision\": 0.7799023596419854, \"recall\": 0.9760692464358453, \"f1-score\": 0.8670284938941655, \"support\": 1964}, \"accuracy\": 0.9050080775444265, \"macro avg\": {\"precision\": 0.8836542880739402, \"recall\": 0.9240261045241223, \"f1-score\": 0.8965702911793019, \"support\": 6190}, \"weighted avg\": {\"precision\": 0.9215681591745996, \"recall\": 0.9050080775444265, \"f1-score\": 0.9073656943229171, \"support\": 6190}}",
  "macro_f1_php_researcher_1": 0.8965702911793019,
  "roc-auc_php_researcher_1": 0.9240261045241224,
  "perf_predict_runtime_php_researcher_1": 0.40864128900102514,
  "timeit_runtime_php_researcher_1": 0.4122378170999582,
  "man_validation_samples_php_researcher_2": 6189,
  "classification_report_php_researcher_2": "{\"artifact\": {\"precision\": 0.9696969696969697, \"recall\": 0.8613625535969509, \"f1-score\": 0.9123249653084395, \"support\": 4198}, \"text\": {\"precision\": 0.7634146341463415, \"recall\": 0.9432446007031643, \"f1-score\": 0.8438553134127162, \"support\": 1991}, \"accuracy\": 0.8877039909516885, \"macro avg\": {\"precision\": 0.8665558019216556, \"recall\": 0.9023035771500576, \"f1-score\": 0.8780901393605779, \"support\": 6189}, \"weighted avg\": {\"precision\": 0.9033359856799557, \"recall\": 0.8877039909516885, \"f1-score\": 0.8902982926756419, \"support\": 6189}}",
  "macro_f1_php_researcher_2": 0.8780901393605779,
  "roc-auc_php_researcher_2": 0.9023035771500576,
  "perf_predict_runtime_php_researcher_2": 0.40221448999909626,
  "timeit_runtime_php_researcher_2": 0.4049991990999843,
  "man_validation_samples_python_researcher_1": 10901,
  "classification_report_python_researcher_1": "{\"artifact\": {\"precision\": 0.9875598086124402, \"recall\": 0.865979381443299, \"f1-score\": 0.92278214217283, \"support\": 8342}, \"text\": {\"precision\": 0.6882320133853876, \"recall\": 0.9644392340758109, \"f1-score\": 0.8032546786004883, \"support\": 2559}, \"accuracy\": 0.8890927437849738, \"macro avg\": {\"precision\": 0.8378959109989139, \"recall\": 0.9152093077595549, \"f1-score\": 0.8630184103866592, \"support\": 10901}, \"weighted avg\": {\"precision\": 0.9172928764056676, \"recall\": 0.8890927437849738, \"f1-score\": 0.8947231770061826, \"support\": 10901}}",
  "macro_f1_python_researcher_1": 0.8630184103866592,
  "roc-auc_python_researcher_1": 0.9152093077595549,
  "perf_predict_runtime_python_researcher_1": 0.7364197540009627,
  "timeit_runtime_python_researcher_1": 0.7300016965000395,
  "man_validation_samples_python_researcher_2": 10983,
  "classification_report_python_researcher_2": "{\"artifact\": {\"precision\": 0.9841742188556742, \"recall\": 0.8634152130058147, \"f1-score\": 0.9198482932996208, \"support\": 8427}, \"text\": {\"precision\": 0.6793871866295265, \"recall\": 0.954225352112676, \"f1-score\": 0.7936869508623495, \"support\": 2556}, \"accuracy\": 0.8845488482199764, \"macro avg\": {\"precision\": 0.8317807027426003, \"recall\": 0.9088202825592453, \"f1-score\": 0.8567676220809852, \"support\": 10983}, \"weighted avg\": {\"precision\": 0.913243175027027, \"recall\": 0.8845488482199764, \"f1-score\": 0.8904876093999883, \"support\": 10983}}",
  "macro_f1_python_researcher_2": 0.8567676220809852,
  "roc-auc_python_researcher_2": 0.9088202825592452,
  "perf_predict_runtime_python_researcher_2": 0.7381212759992195,
  "timeit_runtime_python_researcher_2": 0.7343517094999698
}