{
  "train_samples": 200000,
  "params": "{'charrep__repl_all_caps': False, 'vect__ngram_range': (1, 3), 'vect__stop_words': None, 'vect__lowercase': False}",
  "perf_train_runtime": 73.43538072999945,
  "model_size": 36.90976953125,
  "seed": 42,
  "train_frac": 200000,
  "man_validation_samples_cpp_researcher_1": 5416,
  "classification_report_cpp_researcher_1": "{\"artifact\": {\"precision\": 0.9788227966365618, \"recall\": 0.8476267529665588, \"f1-score\": 0.908512790865732, \"support\": 3708}, \"text\": {\"precision\": 0.7437641723356009, \"recall\": 0.9601873536299765, \"f1-score\": 0.8382315359059546, \"support\": 1708}, \"accuracy\": 0.8831240768094535, \"macro avg\": {\"precision\": 0.8612934844860813, \"recall\": 0.9039070532982676, \"f1-score\": 0.8733721633858433, \"support\": 5416}, \"weighted avg\": {\"precision\": 0.9046942644530238, \"recall\": 0.8831240768094535, \"f1-score\": 0.8863487614212527, \"support\": 5416}}",
  "macro_f1_cpp_researcher_1": 0.8733721633858433,
  "roc-auc_cpp_researcher_1": 0.9039070532982676,
  "perf_predict_runtime_cpp_researcher_1": 0.45971201499924064,
  "timeit_runtime_cpp_researcher_1": 0.46452393300023687,
  "man_validation_samples_cpp_researcher_2": 5418,
  "classification_report_cpp_researcher_2": "{\"artifact\": {\"precision\": 0.9760199314855186, \"recall\": 0.8426996504436677, \"f1-score\": 0.9044733044733044, \"support\": 3719}, \"text\": {\"precision\": 0.7349342999546896, \"recall\": 0.9546792230723955, \"f1-score\": 0.8305171530977982, \"support\": 1699}, \"accuracy\": 0.8778146917681802, \"macro avg\": {\"precision\": 0.8554771157201041, \"recall\": 0.8986894367580316, \"f1-score\": 0.8674952287855513, \"support\": 5418}, \"weighted avg\": {\"precision\": 0.9004192507969104, \"recall\": 0.8778146917681802, \"f1-score\": 0.8812818129290103, \"support\": 5418}}",
  "macro_f1_cpp_researcher_2": 0.8674952287855513,
  "roc-auc_cpp_researcher_2": 0.8986894367580316,
  "perf_predict_runtime_cpp_researcher_2": 0.4715297759976238,
  "timeit_runtime_cpp_researcher_2": 0.4730796299001668,
  "man_validation_samples_java_researcher_1": 6575,
  "classification_report_java_researcher_1": "{\"artifact\": {\"precision\": 0.9846226638277739, \"recall\": 0.8877986348122867, \"f1-score\": 0.9337072349971958, \"support\": 4688}, \"text\": {\"precision\": 0.7759795570698467, \"recall\": 0.9655537890832009, \"f1-score\": 0.8604486422668242, \"support\": 1887}, \"accuracy\": 0.9101140684410647, \"macro avg\": {\"precision\": 0.8803011104488103, \"recall\": 0.9266762119477439, \"f1-score\": 0.89707793863201, \"support\": 6575}, \"weighted avg\": {\"precision\": 0.9247428855080462, \"recall\": 0.9101140684410647, \"f1-score\": 0.9126822974333614, \"support\": 6575}}",
  "macro_f1_java_researcher_1": 0.89707793863201,
  "roc-auc_java_researcher_1": 0.9266762119477439,
  "perf_predict_runtime_java_researcher_1": 0.4420398279980873,
  "timeit_runtime_java_researcher_1": 0.4423097470000357,
  "man_validation_samples_java_researcher_2": 6578,
  "classification_report_java_researcher_2": "{\"artifact\": {\"precision\": 0.9836802270577105, \"recall\": 0.8901969178082192, \"f1-score\": 0.9346067415730337, \"support\": 4672}, \"text\": {\"precision\": 0.7817021276595745, \"recall\": 0.9637985309548793, \"f1-score\": 0.8632518796992481, \"support\": 1906}, \"accuracy\": 0.9115232593493463, \"macro avg\": {\"precision\": 0.8826911773586426, \"recall\": 0.9269977243815493, \"f1-score\": 0.8989293106361409, \"support\": 6578}, \"weighted avg\": {\"precision\": 0.9251563204823309, \"recall\": 0.9115232593493463, \"f1-score\": 0.9139314045813287, \"support\": 6578}}",
  "macro_f1_java_researcher_2": 0.8989293106361409,
  "roc-auc_java_researcher_2": 0.9269977243815493,
  "perf_predict_runtime_java_researcher_2": 0.4405340410012286,
  "timeit_runtime_java_researcher_2": 0.4421976272999018,
  "man_validation_samples_javascript_researcher_1": 5170,
  "classification_report_javascript_researcher_1": "{\"artifact\": {\"precision\": 0.9633635729239358, \"recall\": 0.8521604938271605, \"f1-score\": 0.9043563707828366, \"support\": 3240}, \"text\": {\"precision\": 0.7921006944444444, \"recall\": 0.9455958549222798, \"f1-score\": 0.8620689655172413, \"support\": 1930}, \"accuracy\": 0.8870406189555126, \"macro avg\": {\"precision\": 0.8777321336841901, \"recall\": 0.8988781743747202, \"f1-score\": 0.883212668150039, \"support\": 5170}, \"weighted avg\": {\"precision\": 0.8994298484625396, \"recall\": 0.8870406189555126, \"f1-score\": 0.8885701634012895, \"support\": 5170}}",
  "macro_f1_javascript_researcher_1": 0.883212668150039,
  "roc-auc_javascript_researcher_1": 0.8988781743747202,
  "perf_predict_runtime_javascript_researcher_1": 0.29818898499797797,
  "timeit_runtime_javascript_researcher_1": 0.2985193798002001,
  "man_validation_samples_javascript_researcher_2": 5167,
  "classification_report_javascript_researcher_2": "{\"artifact\": {\"precision\": 0.9633123689727463, \"recall\": 0.8548837209302326, \"f1-score\": 0.905864958107442, \"support\": 3225}, \"text\": {\"precision\": 0.7969631236442516, \"recall\": 0.9459320288362513, \"f1-score\": 0.8650812338121028, \"support\": 1942}, \"accuracy\": 0.8891039287787884, \"macro avg\": {\"precision\": 0.880137746308499, \"recall\": 0.9004078748832419, \"f1-score\": 0.8854730959597724, \"support\": 5167}, \"weighted avg\": {\"precision\": 0.900790550813672, \"recall\": 0.8891039287787884, \"f1-score\": 0.8905365291193351, \"support\": 5167}}",
  "macro_f1_javascript_researcher_2": 0.8854730959597724,
  "roc-auc_javascript_researcher_2": 0.9004078748832419,
  "perf_predict_runtime_javascript_researcher_2": 0.30227686000216636,
  "timeit_runtime_javascript_researcher_2": 0.29556094129984556,
  "man_validation_samples_php_researcher_1": 6190,
  "classification_report_php_researcher_1": "{\"artifact\": {\"precision\": 0.987651077246453, \"recall\": 0.8894936109796497, \"f1-score\": 0.9360059760956175, \"support\": 4226}, \"text\": {\"precision\": 0.8041107382550335, \"recall\": 0.9760692464358453, \"f1-score\": 0.8817847286108556, \"support\": 1964}, \"accuracy\": 0.9169628432956382, \"macro avg\": {\"precision\": 0.8958809077507432, \"recall\": 0.9327814287077475, \"f1-score\": 0.9088953523532366, \"support\": 6190}, \"weighted avg\": {\"precision\": 0.9294163073305971, \"recall\": 0.9169628432956382, \"f1-score\": 0.9188023363443942, \"support\": 6190}}",
  "macro_f1_php_researcher_1": 0.9088953523532366,
  "roc-auc_php_researcher_1": 0.9327814287077475,
  "perf_predict_runtime_php_researcher_1": 0.3695051729991974,
  "timeit_runtime_php_researcher_1": 0.3652900025001145,
  "man_validation_samples_php_researcher_2": 6189,
  "classification_report_php_researcher_2": "{\"artifact\": {\"precision\": 0.9684625492772667, \"recall\": 0.8777989518818485, \"f1-score\": 0.9209046607522179, \"support\": 4198}, \"text\": {\"precision\": 0.7848154362416108, \"recall\": 0.939728779507785, \"f1-score\": 0.8553142857142857, \"support\": 1991}, \"accuracy\": 0.8977217644207465, \"macro avg\": {\"precision\": 0.8766389927594387, \"recall\": 0.9087638656948167, \"f1-score\": 0.8881094732332517, \"support\": 6189}, \"weighted avg\": {\"precision\": 0.9093833115887886, \"recall\": 0.8977217644207465, \"f1-score\": 0.8998042508797793, \"support\": 6189}}",
  "macro_f1_php_researcher_2": 0.8881094732332517,
  "roc-auc_php_researcher_2": 0.9087638656948169,
  "perf_predict_runtime_php_researcher_2": 0.35819048199846293,
  "timeit_runtime_php_researcher_2": 0.3600674822999281,
  "man_validation_samples_python_researcher_1": 10901,
  "classification_report_python_researcher_1": "{\"artifact\": {\"precision\": 0.9746000561324726, \"recall\": 0.8325341644689523, \"f1-score\": 0.8979829325058185, \"support\": 8342}, \"text\": {\"precision\": 0.6299337748344371, \"recall\": 0.9292692457991403, \"f1-score\": 0.7508683296495107, \"support\": 2559}, \"accuracy\": 0.8552426382900651, \"macro avg\": {\"precision\": 0.8022669154834549, \"recall\": 0.8809017051340463, \"f1-score\": 0.8244256310776645, \"support\": 10901}, \"weighted avg\": {\"precision\": 0.8936899548718843, \"recall\": 0.8552426382900651, \"f1-score\": 0.863447911066566, \"support\": 10901}}",
  "macro_f1_python_researcher_1": 0.8244256310776645,
  "roc-auc_python_researcher_1": 0.8809017051340464,
  "perf_predict_runtime_python_researcher_1": 0.6461091800010763,
  "timeit_runtime_python_researcher_1": 0.6476050762998057,
  "man_validation_samples_python_researcher_2": 10983,
  "classification_report_python_researcher_2": "{\"artifact\": {\"precision\": 0.9705759888965996, \"recall\": 0.8298326806692773, \"f1-score\": 0.8947031729785055, \"support\": 8427}, \"text\": {\"precision\": 0.6204340921122287, \"recall\": 0.917057902973396, \"f1-score\": 0.7401326176191979, \"support\": 2556}, \"accuracy\": 0.8501320222161522, \"macro avg\": {\"precision\": 0.7955050405044142, \"recall\": 0.8734452918213367, \"f1-score\": 0.8174178952988518, \"support\": 10983}, \"weighted avg\": {\"precision\": 0.8890898113330147, \"recall\": 0.8501320222161522, \"f1-score\": 0.858731003307342, \"support\": 10983}}",
  "macro_f1_python_researcher_2": 0.8174178952988518,
  "roc-auc_python_researcher_2": 0.8734452918213366,
  "perf_predict_runtime_python_researcher_2": 0.6450046799982374,
  "timeit_runtime_python_researcher_2": 0.6522899552001036
}