{
  "train_samples": 200000,
  "params": "{'charrep__repl_all_caps': False, 'vect__ngram_range': (1, 3), 'vect__stop_words': None, 'vect__lowercase': False}",
  "perf_train_runtime": 56.168659922997904,
  "model_size": 42.076650390625,
  "seed": 42,
  "train_frac": 200000,
  "man_validation_samples_cpp_researcher_1": 5416,
  "classification_report_cpp_researcher_1": "{\"artifact\": {\"precision\": 0.9809693725840024, \"recall\": 0.889697950377562, \"f1-score\": 0.9331070569933531, \"support\": 3708}, \"text\": {\"precision\": 0.800779347296639, \"recall\": 0.9625292740046838, \"f1-score\": 0.8742355756447754, \"support\": 1708}, \"accuracy\": 0.9126661742983752, \"macro avg\": {\"precision\": 0.8908743599403207, \"recall\": 0.9261136121911229, \"f1-score\": 0.9036713163190642, \"support\": 5416}, \"weighted avg\": {\"precision\": 0.9241443055251367, \"recall\": 0.9126661742983752, \"f1-score\": 0.914541235327295, \"support\": 5416}}",
  "macro_f1_cpp_researcher_1": 0.9036713163190642,
  "roc-auc_cpp_researcher_1": 0.926113612191123,
  "perf_predict_runtime_cpp_researcher_1": 0.4899868950014934,
  "timeit_runtime_cpp_researcher_1": 0.5019332610001583,
  "man_validation_samples_cpp_researcher_2": 5418,
  "classification_report_cpp_researcher_2": "{\"artifact\": {\"precision\": 0.9807635395087304, \"recall\": 0.8910997579994622, \"f1-score\": 0.9337841645533953, \"support\": 3719}, \"text\": {\"precision\": 0.8013732221677293, \"recall\": 0.9617422012948793, \"f1-score\": 0.8742643124665597, \"support\": 1699}, \"accuracy\": 0.9132521225544481, \"macro avg\": {\"precision\": 0.8910683808382298, \"recall\": 0.9264209796471707, \"f1-score\": 0.9040242385099775, \"support\": 5418}, \"weighted avg\": {\"precision\": 0.9245095437238724, \"recall\": 0.9132521225544481, \"f1-score\": 0.9151196705158291, \"support\": 5418}}",
  "macro_f1_cpp_researcher_2": 0.9040242385099775,
  "roc-auc_cpp_researcher_2": 0.9264209796471707,
  "perf_predict_runtime_cpp_researcher_2": 0.5053131079985178,
  "timeit_runtime_cpp_researcher_2": 0.5088063776998751,
  "man_validation_samples_java_researcher_1": 6575,
  "classification_report_java_researcher_1": "{\"artifact\": {\"precision\": 0.9867595818815331, \"recall\": 0.9061433447098977, \"f1-score\": 0.9447347937284555, \"support\": 4688}, \"text\": {\"precision\": 0.8061674008810573, \"recall\": 0.9697933227344993, \"f1-score\": 0.880442626894395, \"support\": 1887}, \"accuracy\": 0.9244106463878327, \"macro avg\": {\"precision\": 0.8964634913812952, \"recall\": 0.9379683337221985, \"f1-score\": 0.9125887103114252, \"support\": 6575}, \"weighted avg\": {\"precision\": 0.9349303125966817, \"recall\": 0.9244106463878327, \"f1-score\": 0.9262831863039881, \"support\": 6575}}",
  "macro_f1_java_researcher_1": 0.9125887103114252,
  "roc-auc_java_researcher_1": 0.9379683337221986,
  "perf_predict_runtime_java_researcher_1": 0.48748858500039205,
  "timeit_runtime_java_researcher_1": 0.4897120230998553,
  "man_validation_samples_java_researcher_2": 6578,
  "classification_report_java_researcher_2": "{\"artifact\": {\"precision\": 0.9846761086603204, \"recall\": 0.9077482876712328, \"f1-score\": 0.9446486245684375, \"support\": 4672}, \"text\": {\"precision\": 0.8102157639806252, \"recall\": 0.9653725078698846, \"f1-score\": 0.8810150825951639, \"support\": 1906}, \"accuracy\": 0.924445120097294, \"macro avg\": {\"precision\": 0.8974459363204728, \"recall\": 0.9365603977705588, \"f1-score\": 0.9128318535818007, \"support\": 6578}, \"weighted avg\": {\"precision\": 0.9341255740054863, \"recall\": 0.924445120097294, \"f1-score\": 0.9262105687762424, \"support\": 6578}}",
  "macro_f1_java_researcher_2": 0.9128318535818007,
  "roc-auc_java_researcher_2": 0.9365603977705588,
  "perf_predict_runtime_java_researcher_2": 0.4696693160003633,
  "timeit_runtime_java_researcher_2": 0.4871873683998274,
  "man_validation_samples_javascript_researcher_1": 5170,
  "classification_report_javascript_researcher_1": "{\"artifact\": {\"precision\": 0.9777141037886024, \"recall\": 0.9478395061728395, \"f1-score\": 0.9625450556339131, \"support\": 3240}, \"text\": {\"precision\": 0.9167077378018729, \"recall\": 0.9637305699481865, \"f1-score\": 0.9396312200050517, \"support\": 1930}, \"accuracy\": 0.9537717601547389, \"macro avg\": {\"precision\": 0.9472109207952376, \"recall\": 0.9557850380605131, \"f1-score\": 0.9510881378194824, \"support\": 5170}, \"weighted avg\": {\"precision\": 0.9549399671629955, \"recall\": 0.9537717601547389, \"f1-score\": 0.9539911479426747, \"support\": 5170}}",
  "macro_f1_javascript_researcher_1": 0.9510881378194824,
  "roc-auc_javascript_researcher_1": 0.955785038060513,
  "perf_predict_runtime_javascript_researcher_1": 0.3096360720010125,
  "timeit_runtime_javascript_researcher_1": 0.3369840701998328,
  "man_validation_samples_javascript_researcher_2": 5167,
  "classification_report_javascript_researcher_2": "{\"artifact\": {\"precision\": 0.9780044628626076, \"recall\": 0.9513178294573643, \"f1-score\": 0.9644765796919208, \"support\": 3225}, \"text\": {\"precision\": 0.9226600985221675, \"recall\": 0.9644696189495365, \"f1-score\": 0.943101711983887, \"support\": 1942}, \"accuracy\": 0.9562608863944262, \"macro avg\": {\"precision\": 0.9503322806923875, \"recall\": 0.9578937242034504, \"f1-score\": 0.953789145837904, \"support\": 5167}, \"weighted avg\": {\"precision\": 0.9572034650787611, \"recall\": 0.9562608863944262, \"f1-score\": 0.9564429057826889, \"support\": 5167}}",
  "macro_f1_javascript_researcher_2": 0.953789145837904,
  "roc-auc_javascript_researcher_2": 0.9578937242034504,
  "perf_predict_runtime_javascript_researcher_2": 0.3209292510000523,
  "timeit_runtime_javascript_researcher_2": 0.332627389299887,
  "man_validation_samples_php_researcher_1": 6190,
  "classification_report_php_researcher_1": "{\"artifact\": {\"precision\": 0.9797149122807017, \"recall\": 0.8457169900615239, \"f1-score\": 0.9077978155956312, \"support\": 4226}, \"text\": {\"precision\": 0.7435090479937058, \"recall\": 0.9623217922606925, \"f1-score\": 0.8388814913448734, \"support\": 1964}, \"accuracy\": 0.8827140549273021, \"macro avg\": {\"precision\": 0.8616119801372037, \"recall\": 0.9040193911611082, \"f1-score\": 0.8733396534702523, \"support\": 6190}, \"weighted avg\": {\"precision\": 0.9047701113986887, \"recall\": 0.8827140549273021, \"f1-score\": 0.8859316345247931, \"support\": 6190}}",
  "macro_f1_php_researcher_1": 0.8733396534702523,
  "roc-auc_php_researcher_1": 0.9040193911611082,
  "perf_predict_runtime_php_researcher_1": 0.3996894310002972,
  "timeit_runtime_php_researcher_1": 0.36404736779986707,
  "man_validation_samples_php_researcher_2": 6189,
  "classification_report_php_researcher_2": "{\"artifact\": {\"precision\": 0.9780641623251988, \"recall\": 0.8496903287279657, \"f1-score\": 0.9093690248565965, \"support\": 4198}, \"text\": {\"precision\": 0.7517702596380803, \"recall\": 0.9598191863385234, \"f1-score\": 0.8431502316346791, \"support\": 1991}, \"accuracy\": 0.8851187590887057, \"macro avg\": {\"precision\": 0.8649172109816395, \"recall\": 0.9047547575332445, \"f1-score\": 0.8762596282456379, \"support\": 6189}, \"weighted avg\": {\"precision\": 0.9052654613638073, \"recall\": 0.8851187590887057, \"f1-score\": 0.8880664529863692, \"support\": 6189}}",
  "macro_f1_php_researcher_2": 0.8762596282456379,
  "roc-auc_php_researcher_2": 0.9047547575332445,
  "perf_predict_runtime_php_researcher_2": 0.35918992499864544,
  "timeit_runtime_php_researcher_2": 0.4076216300996748,
  "man_validation_samples_python_researcher_1": 10901,
  "classification_report_python_researcher_1": "{\"artifact\": {\"precision\": 0.9817780231916069, \"recall\": 0.8525533445216974, \"f1-score\": 0.9126138842551007, \"support\": 8342}, \"text\": {\"precision\": 0.6636587366694011, \"recall\": 0.9484173505275498, \"f1-score\": 0.7808880308880308, \"support\": 2559}, \"accuracy\": 0.8750573341895239, \"macro avg\": {\"precision\": 0.822718379930504, \"recall\": 0.9004853475246236, \"f1-score\": 0.8467509575715657, \"support\": 10901}, \"weighted avg\": {\"precision\": 0.907099805210658, \"recall\": 0.8750573341895239, \"f1-score\": 0.8816913579945438, \"support\": 10901}}",
  "macro_f1_python_researcher_1": 0.8467509575715657,
  "roc-auc_python_researcher_1": 0.9004853475246235,
  "perf_predict_runtime_python_researcher_1": 0.727080834996741,
  "timeit_runtime_python_researcher_1": 0.7185084745000495,
  "man_validation_samples_python_researcher_2": 10983,
  "classification_report_python_researcher_2": "{\"artifact\": {\"precision\": 0.9784270890223922, \"recall\": 0.8503619318856058, \"f1-score\": 0.9099104818741668, \"support\": 8427}, \"text\": {\"precision\": 0.6553703197594971, \"recall\": 0.9381846635367762, \"f1-score\": 0.7716814159292036, \"support\": 2556}, \"accuracy\": 0.8708003277792953, \"macro avg\": {\"precision\": 0.8168987043909446, \"recall\": 0.894273297711191, \"f1-score\": 0.8407959489016852, \"support\": 10983}, \"weighted avg\": {\"precision\": 0.9032442517069083, \"recall\": 0.8708003277792953, \"f1-score\": 0.8777413575406217, \"support\": 10983}}",
  "macro_f1_python_researcher_2": 0.8407959489016852,
  "roc-auc_python_researcher_2": 0.8942732977111911,
  "perf_predict_runtime_python_researcher_2": 0.7023781659991073,
  "timeit_runtime_python_researcher_2": 0.7000343873001839
}